{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aydan/Desktop/vu/Semestr 1 Period 2/Web Data Processing/untitled folder/env/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from collections import Counter\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torchcrf\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "from torch.utils.tensorboard import SummaryWriter \n",
    "from sklearn.metrics import classification_report\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### CONLL 2003 NER DATASET ####\n",
    "# Loading the dataset from huggingface\n",
    "dataset = load_dataset(\"eriktks/conll2003\", trust_remote_code=True)\n",
    "\n",
    "def build_vocab(dataset, min_freq=1):\n",
    "    \"\"\" \n",
    "    Build a vocabulary from the dataset.\n",
    "    \n",
    "    Args:\n",
    "        dataset: a dataset object\n",
    "        min_freq: minimum frequency of a word to be included in the vocabulary\n",
    "    Returns:\n",
    "        word_vocab: a dictionary with words as keys and indices as values\n",
    "    \"\"\"\n",
    "    word_freq = Counter()\n",
    "    \n",
    "    for example in dataset['train']:\n",
    "        word_freq.update(example[\"tokens\"])  # Counting word occurrences\n",
    "    \n",
    "    # Assigning the indices to words (PAD=0, UNK=1)\n",
    "    word_vocab = {\"<PAD>\": 0, \"<UNK>\": 1}\n",
    "    for word, freq in word_freq.items():\n",
    "        if freq >= min_freq:\n",
    "            word_vocab[word] = len(word_vocab)\n",
    "    \n",
    "    return word_vocab\n",
    "\n",
    "word_vocab = build_vocab(dataset)\n",
    "\n",
    "# NER labels\n",
    "label_vocab = {'O': 0, 'B-PER': 1, 'I-PER': 2, 'B-ORG': 3, 'I-ORG': 4, 'B-LOC': 5, 'I-LOC': 6, 'B-MISC': 7, 'I-MISC': 8}\n",
    "def encode_sentence(tokens, vocab, max_len=128):\n",
    "    \"\"\"\n",
    "    Encode a sentence of tokens into indices using a vocabulary.\n",
    "    Truncate to the maximum length if longer, or pad with zeros if shorter.\n",
    "    \n",
    "    Args:\n",
    "        tokens (List[str]): List of tokens in the sentence\n",
    "        vocab (Dict[str, int]): Mapping from token to index\n",
    "        max_len (int): Maximum length of the output sequence\n",
    "    Returns:\n",
    "        indices (torch.Tensor): Tensor of token indices\n",
    "    \"\"\"\n",
    "    \n",
    "    indices = [vocab.get(word, vocab[\"<UNK>\"]) for word in tokens]  \n",
    "    indices = indices[:max_len] # Truncating\n",
    "    indices += [word_vocab[\"<PAD>\"]] * (max_len - len(indices)) # Padding\n",
    "    return torch.tensor(indices, dtype=torch.long) \n",
    "\n",
    "def encode_labels(labels, max_len=128):\n",
    "    \"\"\" \n",
    "    Encode a sequence of NER labels into indices.\n",
    "    Truncate to the maximum length if longer, or pad with -100 if shorter.\n",
    "    \n",
    "    Args:\n",
    "        labels (List[str]): List of NER labels\n",
    "        max_len (int): Maximum length of the output sequence\n",
    "    Returns:\n",
    "        indices (torch.Tensor): Tensor of label indices\n",
    "    \"\"\"\n",
    "    \n",
    "    indices = labels[:max_len] # Truncating\n",
    "    indices += [-100] * (max_len - len(indices))  # Padding\n",
    "    return torch.tensor(indices, dtype=torch.long)  \n",
    "\n",
    "def prepare_dataset(dataset, word_vocab, max_len=128):\n",
    "    \"\"\" \n",
    "    Prepare the dataset by encoding the tokens and labels.\n",
    "    \n",
    "    Args:\n",
    "        dataset: a dataset object\n",
    "        word_vocab: a dictionary with words as keys and indices as values\n",
    "        max_len: maximum length of the input sequences\n",
    "    Returns:\n",
    "        data: a list of tuples, each containing a tensor of token indices and a tensor of label indices\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    \n",
    "    for example in dataset:\n",
    "        token_ids = encode_sentence(example[\"tokens\"], word_vocab, max_len)\n",
    "        label_ids = encode_labels(example[\"ner_tags\"], max_len)\n",
    "        data.append((token_ids, label_ids))\n",
    "    \n",
    "    return data\n",
    "\n",
    "train_data = prepare_dataset(dataset[\"train\"], word_vocab)\n",
    "val_data = prepare_dataset(dataset[\"validation\"], word_vocab)\n",
    "test_data = prepare_dataset(dataset[\"test\"], word_vocab)\n",
    "\n",
    "# dataset class\n",
    "class NERDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        # Return the token indices and label indices\n",
    "        return self.data[index] \n",
    "\n",
    "train_dataset = NERDataset(train_data)\n",
    "val_dataset = NERDataset(val_data)\n",
    "test_dataset = NERDataset(test_data)\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size: 131767 sentences\n",
      "Validation dataset size: 18824 sentences\n",
      "Test dataset size: 37648 sentences\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"DFKI-SLT/few-nerd\", \"supervised\", trust_remote_code=True)\n",
    "\n",
    "# entity labels\n",
    "label_list = dataset['train'].features['ner_tags'].feature.names\n",
    "label2id = {label: i for i, label in enumerate(label_list)}\n",
    "id2label = {i: label for label, i in label2id.items()}\n",
    "\n",
    "# Building the vocabulary\n",
    "def build_vocab(dataset, min_freq=1):\n",
    "    word_freq = Counter()\n",
    "    \n",
    "    for example in dataset['train']:\n",
    "        word_freq.update(example[\"tokens\"])\n",
    "    \n",
    "    word_vocab = {\"<PAD>\": 0, \"<UNK>\": 1}\n",
    "    for word, count in word_freq.items():\n",
    "        if count >= min_freq:\n",
    "            word_vocab[word] = len(word_vocab)\n",
    "    \n",
    "    return word_vocab\n",
    "\n",
    "word_vocab = build_vocab(dataset)\n",
    "\n",
    "class NERDataset(Dataset):\n",
    "    def __init__(self, dataset_split, word_vocab, label2id, max_length=128):\n",
    "        self.dataset = dataset_split\n",
    "        self.word_vocab = word_vocab\n",
    "        self.label2id = label2id\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        example = self.dataset[idx]\n",
    "        tokens = example['tokens']\n",
    "        labels = example['ner_tags']\n",
    "        \n",
    "        # Converting tokens to indices\n",
    "        input_ids = [self.word_vocab.get(token, self.word_vocab[\"<UNK>\"]) for token in tokens]\n",
    "        label_ids = [self.label2id[label_list[label]] for label in labels]\n",
    "        \n",
    "        # Padding\n",
    "        if len(input_ids) < self.max_length:\n",
    "            pad_length = self.max_length - len(input_ids)\n",
    "            input_ids += [self.word_vocab[\"<PAD>\"]] * pad_length\n",
    "            label_ids += [-100] * pad_length  # -100 for ignored tokens in loss function\n",
    "        else:\n",
    "            input_ids = input_ids[:self.max_length]\n",
    "            label_ids = label_ids[:self.max_length]\n",
    "        \n",
    "        return torch.tensor(input_ids), torch.tensor(label_ids)\n",
    "\n",
    "# DataLoaders\n",
    "train_dataset = NERDataset(dataset['train'], word_vocab, label2id)\n",
    "val_dataset = NERDataset(dataset['validation'], word_vocab, label2id)\n",
    "test_dataset = NERDataset(dataset['test'], word_vocab, label2id)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "print(f\"Train dataset size: {len(train_dataset)} sentences\")\n",
    "print(f\"Validation dataset size: {len(val_dataset)} sentences\")\n",
    "print(f\"Test dataset size: {len(test_dataset)} sentences\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model class\n",
    "class BiLSTM_CRF(nn.Module):\n",
    "    def __init__(self, vocab_size, label_size, embedding_dim=100, hidden_dim=128, dropout=0.5):\n",
    "        super(BiLSTM_CRF, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim // 2, num_layers=2, \n",
    "                            bidirectional=True, batch_first=True, dropout=dropout)\n",
    "        \n",
    "        self.fc = nn.Linear(hidden_dim, label_size)\n",
    "        self.crf = torchcrf.CRF(label_size, batch_first=True)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        x = self.embedding(x)\n",
    "        x, _ = self.lstm(x)\n",
    "        emissions = self.fc(x)\n",
    "        return emissions, mask\n",
    "    \n",
    "    def loss(self, x, tags, mask):\n",
    "        emissions, mask = self.forward(x, mask)\n",
    "        tags = torch.where(tags == -100, torch.tensor(0, device=tags.device), tags)\n",
    "\n",
    "        return -self.crf(emissions, tags, mask=mask, reduction='mean')\n",
    "\n",
    "    def predict(self, x, mask):\n",
    "        emissions, mask = self.forward(x, mask)\n",
    "        return self.crf.decode(emissions, mask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 Training: 100%|██████████| 4118/4118 [09:26<00:00,  7.27it/s]\n",
      "Epoch 1 Validation: 100%|██████████| 589/589 [00:44<00:00, 13.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1:\n",
      "Training Loss: 8.0979\n",
      "Validation Loss: 4.2975 | Validation Accuracy: 0.9199\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 Training: 100%|██████████| 4118/4118 [09:37<00:00,  7.14it/s]\n",
      "Epoch 2 Validation: 100%|██████████| 589/589 [00:43<00:00, 13.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2:\n",
      "Training Loss: 3.7337\n",
      "Validation Loss: 3.4408 | Validation Accuracy: 0.9303\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 Training: 100%|██████████| 4118/4118 [28:26<00:00,  2.41it/s]    \n",
      "Epoch 3 Validation: 100%|██████████| 589/589 [04:45<00:00,  2.06it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3:\n",
      "Training Loss: 3.0383\n",
      "Validation Loss: 3.3269 | Validation Accuracy: 0.9318\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 Training: 100%|██████████| 4118/4118 [1:00:14<00:00,  1.14it/s]   \n",
      "Epoch 4 Validation: 100%|██████████| 589/589 [00:43<00:00, 13.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4:\n",
      "Training Loss: 2.7520\n",
      "Validation Loss: 3.1599 | Validation Accuracy: 0.9347\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5 Training: 100%|██████████| 4118/4118 [1:12:44<00:00,  1.06s/it]    \n",
      "Epoch 5 Validation: 100%|██████████| 589/589 [00:43<00:00, 13.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 5:\n",
      "Training Loss: 2.5562\n",
      "Validation Loss: 3.1171 | Validation Accuracy: 0.9356\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6 Training: 100%|██████████| 4118/4118 [1:30:41<00:00,  1.32s/it]     \n",
      "Epoch 6 Validation: 100%|██████████| 589/589 [00:43<00:00, 13.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 6:\n",
      "Training Loss: 2.4283\n",
      "Validation Loss: 3.0936 | Validation Accuracy: 0.9356\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7 Training: 100%|██████████| 4118/4118 [42:14<00:00,  1.63it/s]     \n",
      "Epoch 7 Validation: 100%|██████████| 589/589 [00:45<00:00, 12.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 7:\n",
      "Training Loss: 2.3115\n",
      "Validation Loss: 3.0721 | Validation Accuracy: 0.9362\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8 Training: 100%|██████████| 4118/4118 [1:21:11<00:00,  1.18s/it]     \n",
      "Epoch 8 Validation: 100%|██████████| 589/589 [00:43<00:00, 13.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 8:\n",
      "Training Loss: 2.2120\n",
      "Validation Loss: 3.1128 | Validation Accuracy: 0.9361\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9 Training: 100%|██████████| 4118/4118 [1:12:53<00:00,  1.06s/it]   \n",
      "Epoch 9 Validation: 100%|██████████| 589/589 [00:43<00:00, 13.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 9:\n",
      "Training Loss: 2.1183\n",
      "Validation Loss: 3.1005 | Validation Accuracy: 0.9363\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10 Training: 100%|██████████| 4118/4118 [44:36<00:00,  1.54it/s]    \n",
      "Epoch 10 Validation: 100%|██████████| 589/589 [16:15<00:00,  1.66s/it]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 10:\n",
      "Training Loss: 2.0400\n",
      "Validation Loss: 3.1363 | Validation Accuracy: 0.9376\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11 Training: 100%|██████████| 4118/4118 [53:38<00:00,  1.28it/s]    \n",
      "Epoch 11 Validation: 100%|██████████| 589/589 [00:43<00:00, 13.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 11:\n",
      "Training Loss: 1.9638\n",
      "Validation Loss: 3.1123 | Validation Accuracy: 0.9365\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12 Training: 100%|██████████| 4118/4118 [09:30<00:00,  7.21it/s]\n",
      "Epoch 12 Validation: 100%|██████████| 589/589 [00:43<00:00, 13.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 12:\n",
      "Training Loss: 1.8947\n",
      "Validation Loss: 3.2090 | Validation Accuracy: 0.9338\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13 Training: 100%|██████████| 4118/4118 [09:29<00:00,  7.23it/s]\n",
      "Epoch 13 Validation: 100%|██████████| 589/589 [09:19<00:00,  1.05it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 13:\n",
      "Training Loss: 1.8340\n",
      "Validation Loss: 3.2056 | Validation Accuracy: 0.9364\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14 Training: 100%|██████████| 4118/4118 [55:22<00:00,  1.24it/s]     \n",
      "Epoch 14 Validation: 100%|██████████| 589/589 [00:46<00:00, 12.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 14:\n",
      "Training Loss: 1.7756\n",
      "Validation Loss: 3.1878 | Validation Accuracy: 0.9361\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15 Training: 100%|██████████| 4118/4118 [09:46<00:00,  7.02it/s]\n",
      "Epoch 15 Validation: 100%|██████████| 589/589 [00:43<00:00, 13.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 15:\n",
      "Training Loss: 1.7334\n",
      "Validation Loss: 3.3160 | Validation Accuracy: 0.9361\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16 Training: 100%|██████████| 4118/4118 [10:16<00:00,  6.68it/s]\n",
      "Epoch 16 Validation: 100%|██████████| 589/589 [00:45<00:00, 13.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 16:\n",
      "Training Loss: 1.6788\n",
      "Validation Loss: 3.3292 | Validation Accuracy: 0.9361\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17 Training: 100%|██████████| 4118/4118 [09:40<00:00,  7.09it/s]\n",
      "Epoch 17 Validation: 100%|██████████| 589/589 [00:47<00:00, 12.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 17:\n",
      "Training Loss: 1.6300\n",
      "Validation Loss: 3.3182 | Validation Accuracy: 0.9352\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18 Training: 100%|██████████| 4118/4118 [09:26<00:00,  7.27it/s]\n",
      "Epoch 18 Validation: 100%|██████████| 589/589 [00:44<00:00, 13.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 18:\n",
      "Training Loss: 1.5967\n",
      "Validation Loss: 3.3523 | Validation Accuracy: 0.9350\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19 Training: 100%|██████████| 4118/4118 [09:53<00:00,  6.94it/s]\n",
      "Epoch 19 Validation: 100%|██████████| 589/589 [00:45<00:00, 13.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 19:\n",
      "Training Loss: 1.5470\n",
      "Validation Loss: 3.4157 | Validation Accuracy: 0.9333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20 Training: 100%|██████████| 4118/4118 [10:02<00:00,  6.83it/s]\n",
      "Epoch 20 Validation: 100%|██████████| 589/589 [00:46<00:00, 12.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 20:\n",
      "Training Loss: 1.5071\n",
      "Validation Loss: 3.4432 | Validation Accuracy: 0.9345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# TensorBoard\n",
    "writer = SummaryWriter(\"runs/ner_training\")  # Logs will be saved in this directory\n",
    "\n",
    "model = BiLSTM_CRF(vocab_size=len(word_vocab), label_size=len(id2label)).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "\n",
    "num_epochs = 20  \n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train() # training mode\n",
    "    total_train_loss = 0\n",
    "\n",
    "    # tqdm progress bar\n",
    "    for tokens, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1} Training\"):\n",
    "        tokens, labels = tokens.to(device), labels.to(device)\n",
    "        mask = tokens != word_vocab[\"<PAD>\"]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss = model.loss(tokens, labels, mask)  # loss computation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_train_loss += loss.item()  # training loss\n",
    "\n",
    "    avg_train_loss = total_train_loss / len(train_loader) \n",
    "    writer.add_scalar(\"Loss/Train\", avg_train_loss, epoch) \n",
    "\n",
    "    # Validation\n",
    "    model.eval()  # evaluation mode\n",
    "    total_val_loss = 0\n",
    "    correct_val = 0\n",
    "    total_val = 0\n",
    "\n",
    "    with torch.no_grad():  # No gradient updates for validation\n",
    "        for tokens, labels in tqdm(val_loader, desc=f\"Epoch {epoch+1} Validation\"):\n",
    "            tokens, labels = tokens.to(device), labels.to(device)\n",
    "            mask = tokens != word_vocab[\"<PAD>\"]\n",
    "\n",
    "            loss = model.loss(tokens, labels, mask)  # validation loss\n",
    "            total_val_loss += loss.item()\n",
    "\n",
    "            # predictions\n",
    "            predictions = model.predict(tokens, mask)\n",
    "            labels_masked = labels[mask]\n",
    "\n",
    "            predictions_masked = torch.cat([torch.tensor(p, dtype=torch.long, device=device) for p in predictions])\n",
    "\n",
    "            if predictions_masked.shape != labels_masked.shape:\n",
    "                min_len = min(predictions_masked.shape[0], labels_masked.shape[0])\n",
    "                predictions_masked = predictions_masked[:min_len]\n",
    "                labels_masked = labels_masked[:min_len]\n",
    "\n",
    "            correct_val += (predictions_masked == labels_masked).sum().item()\n",
    "            total_val += len(labels_masked)\n",
    "\n",
    "    avg_val_loss = total_val_loss / len(val_loader) \n",
    "    val_accuracy = correct_val / total_val if total_val > 0 else 0\n",
    "\n",
    "    writer.add_scalar(\"Loss/Validation\", avg_val_loss, epoch) \n",
    "    writer.add_scalar(\"Accuracy/Validation\", val_accuracy, epoch) \n",
    "\n",
    "    print(f\"\\nEpoch {epoch+1}:\")\n",
    "    print(f\"Training Loss: {avg_train_loss:.4f}\")\n",
    "    print(f\"Validation Loss: {avg_val_loss:.4f} | Validation Accuracy: {val_accuracy:.4f}\")\n",
    "\n",
    "# Close TensorBoard writer\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow installation not found - running with reduced feature set.\n",
      "Serving TensorBoard on localhost; to expose to the network, use a proxy or pass --bind_all\n",
      "TensorBoard 2.18.0 at http://localhost:6006/ (Press CTRL+C to quit)\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "!tensorboard --logdir=runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 1177/1177 [00:46<00:00, 25.46it/s]\n"
     ]
    }
   ],
   "source": [
    "model.eval() \n",
    "\n",
    "true_labels = []\n",
    "predicted_labels = []\n",
    "\n",
    "with torch.no_grad():  # No gradient updates for testing\n",
    "    for tokens, labels in tqdm(test_loader, desc=\"Testing\"):\n",
    "        tokens, labels = tokens.to(device), labels.to(device)\n",
    "        mask = tokens != word_vocab[\"<PAD>\"]\n",
    "\n",
    "        predictions = model.predict(tokens, mask)\n",
    "\n",
    "        for i in range(len(labels)):  \n",
    "            # Get true labels, ignoring padding (-100)\n",
    "            true_seq = labels[i][mask[i]].cpu().tolist()\n",
    "            pred_seq = predictions[i]  # Predictions are already masked\n",
    "\n",
    "            true_labels.extend(true_seq)\n",
    "            predicted_labels.extend(pred_seq)\n",
    "\n",
    "# # label indices to actual class names\n",
    "idx_to_label = {idx: label for label, idx in id2label.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1-score</th>\n",
       "      <th>support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.970682</td>\n",
       "      <td>0.977903</td>\n",
       "      <td>0.974279</td>\n",
       "      <td>727795.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.780423</td>\n",
       "      <td>0.771687</td>\n",
       "      <td>0.776030</td>\n",
       "      <td>12150.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.671140</td>\n",
       "      <td>0.741578</td>\n",
       "      <td>0.704603</td>\n",
       "      <td>13892.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.697083</td>\n",
       "      <td>0.733021</td>\n",
       "      <td>0.714601</td>\n",
       "      <td>12162.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.816825</td>\n",
       "      <td>0.831975</td>\n",
       "      <td>0.824330</td>\n",
       "      <td>40976.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.788814</td>\n",
       "      <td>0.740906</td>\n",
       "      <td>0.764110</td>\n",
       "      <td>46103.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.759426</td>\n",
       "      <td>0.614216</td>\n",
       "      <td>0.679146</td>\n",
       "      <td>17839.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.883026</td>\n",
       "      <td>0.885280</td>\n",
       "      <td>0.884151</td>\n",
       "      <td>37221.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.735641</td>\n",
       "      <td>0.619359</td>\n",
       "      <td>0.672510</td>\n",
       "      <td>12697.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.934290</td>\n",
       "      <td>0.934290</td>\n",
       "      <td>0.934290</td>\n",
       "      <td>0.93429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>macro avg</th>\n",
       "      <td>0.789229</td>\n",
       "      <td>0.768436</td>\n",
       "      <td>0.777085</td>\n",
       "      <td>920835.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weighted avg</th>\n",
       "      <td>0.933211</td>\n",
       "      <td>0.934290</td>\n",
       "      <td>0.933449</td>\n",
       "      <td>920835.00000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              precision    recall  f1-score       support\n",
       "0              0.970682  0.977903  0.974279  727795.00000\n",
       "1              0.780423  0.771687  0.776030   12150.00000\n",
       "2              0.671140  0.741578  0.704603   13892.00000\n",
       "3              0.697083  0.733021  0.714601   12162.00000\n",
       "4              0.816825  0.831975  0.824330   40976.00000\n",
       "5              0.788814  0.740906  0.764110   46103.00000\n",
       "6              0.759426  0.614216  0.679146   17839.00000\n",
       "7              0.883026  0.885280  0.884151   37221.00000\n",
       "8              0.735641  0.619359  0.672510   12697.00000\n",
       "accuracy       0.934290  0.934290  0.934290       0.93429\n",
       "macro avg      0.789229  0.768436  0.777085  920835.00000\n",
       "weighted avg   0.933211  0.934290  0.933449  920835.00000"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.DataFrame(classification_report(true_labels, predicted_labels, output_dict=True)).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"bilstm_crf_ner_few-nerd10.pth\"  \n",
    "vocab_path = \"vocab_few-nerd10.pkl\"  \n",
    "\n",
    "# saving\n",
    "torch.save(model.state_dict(), model_path)\n",
    "\n",
    "with open(vocab_path, \"wb\") as f:\n",
    "    pickle.dump({\"word_vocab\": word_vocab, \"label_vocab\": id2label}, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/sv/_kgrn0q96tjfh0j08_hlstp00000gn/T/ipykernel_18483/998782864.py:8: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  loaded_model.load_state_dict(torch.load(\"bilstm_crf_ner_few-nerd10.pth\", map_location=device))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BiLSTM_CRF(\n",
       "  (embedding): Embedding(171197, 100)\n",
       "  (lstm): LSTM(100, 64, num_layers=2, batch_first=True, dropout=0.5, bidirectional=True)\n",
       "  (fc): Linear(in_features=128, out_features=9, bias=True)\n",
       "  (crf): CRF(num_tags=9)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(vocab_path, \"rb\") as f:\n",
    "    vocab_data = pickle.load(f)\n",
    "    \n",
    "word_vocab = vocab_data[\"word_vocab\"]\n",
    "label_vocab = vocab_data[\"label_vocab\"]\n",
    "\n",
    "loaded_model = BiLSTM_CRF(vocab_size=len(word_vocab), label_size=len(label_vocab)).to(device)\n",
    "loaded_model.load_state_dict(torch.load(\"bilstm_crf_ner_few-nerd10.pth\", map_location=device))\n",
    "loaded_model.eval() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### CONLL 2003 NER PREDICTION ###\n",
    "def predict_ner(sentence, model, word_vocab, label_vocab):\n",
    "    \"\"\" \n",
    "    Predict NER entities in a sentence using a trained model.\n",
    "    \n",
    "    Args:\n",
    "        sentence (str): Input sentence\n",
    "        model: Trained NER model\n",
    "        word_vocab: Vocabulary for words\n",
    "        label_vocab: Vocabulary for NER labels\n",
    "        \n",
    "    Returns:\n",
    "        entities (Dict[str, List[str]]): Dictionary of NER entities with their types\"\"\"\n",
    "    tokens = sentence.split()\n",
    "\n",
    "    # words to indices \n",
    "    token_ids = [word_vocab.get(word, word_vocab[\"<UNK>\"]) for word in tokens]\n",
    "    token_tensor = torch.tensor([token_ids], dtype=torch.long, device=device)\n",
    "\n",
    "    # mask for valid tokens\n",
    "    mask = torch.ones_like(token_tensor, dtype=torch.bool) \n",
    "\n",
    "    with torch.no_grad():\n",
    "        predictions = model.predict(token_tensor, mask) # predictions\n",
    "\n",
    "    # predictions to labels\n",
    "    idx_to_label = {idx: label for label, idx in label_vocab.items()}\n",
    "    predicted_labels = [idx_to_label[pred] for pred in predictions[0]]\n",
    "\n",
    "    # NER entities into full names such as \"B-PER\" to \"PERSON\"\n",
    "    entities = {}\n",
    "    current_entity = []\n",
    "    current_type = None\n",
    "\n",
    "    for word, label in zip(tokens, predicted_labels):\n",
    "        if label.startswith(\"B-\"):  # beginning of an entity\n",
    "            if current_entity:  \n",
    "                entities.setdefault(current_type, []).append(\" \".join(current_entity))\n",
    "            current_type = label[2:]  \n",
    "            current_entity = [word]  \n",
    "\n",
    "        elif label.startswith(\"I-\") and current_type == label[2:]:  \n",
    "            current_entity.append(word)\n",
    "\n",
    "        else:  # If O or new entity starts, saving previous entity\n",
    "            if current_entity:\n",
    "                entities.setdefault(current_type, []).append(\" \".join(current_entity))\n",
    "                current_entity = []\n",
    "                current_type = None\n",
    "\n",
    "    # last entity handling\n",
    "    if current_entity:\n",
    "        entities.setdefault(current_type, []).append(\" \".join(current_entity))\n",
    "\n",
    "    return entities \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The', 'O'), ('capital', 'O'), ('of', 'O'), ('Nicaragua', 'location'), ('is', 'O'), ('Managua', 'location')]\n"
     ]
    }
   ],
   "source": [
    "### FEW-NERD PREDICTION ###\n",
    "# prediction but without merging the entities\n",
    "def infer(model, sentence, word_vocab, id2label, max_length=128):\n",
    "    model.eval()\n",
    "    tokens = sentence.split()\n",
    "    input_ids = [word_vocab.get(token, word_vocab[\"<UNK>\"]) for token in tokens]\n",
    "    \n",
    "    if len(input_ids) < max_length:\n",
    "        input_ids += [word_vocab[\"<PAD>\"]] * (max_length - len(input_ids))\n",
    "    else:\n",
    "        input_ids = input_ids[:max_length]\n",
    "    \n",
    "    input_tensor = torch.tensor([input_ids])\n",
    "    \n",
    "    mask = (input_tensor != word_vocab[\"<PAD>\"]).long()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = model(input_tensor, mask)\n",
    "    \n",
    "    if isinstance(output, tuple):\n",
    "        output = output[0]  # Extract logits from tuple\n",
    "    \n",
    "    predicted_labels = torch.argmax(output, dim=2).squeeze(0).tolist()\n",
    "    entity_predictions = [id2label[label] for label in predicted_labels[:len(tokens)]]\n",
    "    \n",
    "    return list(zip(tokens, entity_predictions))\n",
    "\n",
    "\n",
    "# Example usage\n",
    "\n",
    "sentence = \"The capital of Nicaragua is Managua\"\n",
    "print(infer(loaded_model, sentence, word_vocab, id2label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Barack Obama', 'person'), ('United', 'organization'), ('States', 'location'), ('America', 'location')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/sv/_kgrn0q96tjfh0j08_hlstp00000gn/T/ipykernel_18483/1087530624.py:45: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"bilstm_crf_ner_few-nerd10.pth\", map_location=device))\n"
     ]
    }
   ],
   "source": [
    "### FEW-NERD PREDICTION ###\n",
    "# prediction but with merging the consecutive entities\n",
    "def infer(model, sentence, word_vocab, id2label, max_length=128):\n",
    "    model.eval()\n",
    "    tokens = sentence.split()\n",
    "    input_ids = [word_vocab.get(token, word_vocab[\"<UNK>\"]) for token in tokens]\n",
    "    \n",
    "    if len(input_ids) < max_length:\n",
    "        input_ids += [word_vocab[\"<PAD>\"]] * (max_length - len(input_ids))\n",
    "    else:\n",
    "        input_ids = input_ids[:max_length]\n",
    "    \n",
    "    input_tensor = torch.tensor([input_ids])\n",
    "    \n",
    "    mask = (input_tensor != word_vocab[\"<PAD>\"]).long()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = model(input_tensor, mask)\n",
    "    \n",
    "    if isinstance(output, tuple):\n",
    "        output = output[0]  # Extract logits from tuple\n",
    "    \n",
    "    predicted_labels = torch.argmax(output, dim=2).squeeze(0).tolist()\n",
    "    entity_predictions = [id2label[label] for label in predicted_labels[:len(tokens)]]\n",
    "    \n",
    "    # Merge consecutive entities of the same type\n",
    "    merged_entities = []\n",
    "    current_entity = None\n",
    "    for token, entity in zip(tokens, entity_predictions):\n",
    "        if entity != 'O':\n",
    "            if current_entity and current_entity[1] == entity:\n",
    "                current_entity[0] += \" \" + token\n",
    "            else:\n",
    "                if current_entity:\n",
    "                    merged_entities.append(tuple(current_entity))\n",
    "                current_entity = [token, entity]\n",
    "        else:\n",
    "            if current_entity:\n",
    "                merged_entities.append(tuple(current_entity))\n",
    "                current_entity = None\n",
    "    if current_entity:\n",
    "        merged_entities.append(tuple(current_entity))\n",
    "    \n",
    "    return merged_entities\n",
    "\n",
    "model = BiLSTM_CRF(vocab_size=len(word_vocab), label_size=len(label_vocab)).to(device)\n",
    "model.load_state_dict(torch.load(\"bilstm_crf_ner_few-nerd10.pth\", map_location=device))\n",
    "model.eval() \n",
    "# Example usage\n",
    "sentence = \"Barack Obama was the president of United States of America\"\n",
    "print(infer(model, sentence, word_vocab, id2label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
