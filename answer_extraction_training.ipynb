{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "rtUXmUOZeNdF"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/aydan/Desktop/vu/Semestr 1 Period 2/Web Data Processing/untitled folder/env/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import DistilBertModel\n",
        "from datasets import load_dataset\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "btQ5zVcyeNdJ"
      },
      "outputs": [],
      "source": [
        "class BLANC(nn.Module):\n",
        "    def __init__(self, model_name=\"distilbert-base-uncased\", freeze_bert=True, dropout_rate=0.3):\n",
        "        super().__init__()\n",
        "        self.bert = DistilBertModel.from_pretrained(model_name)\n",
        "        hidden_size = self.bert.config.hidden_size  # 768 for BERT, 312 for TinyBERT\n",
        "\n",
        "        # Dropout for regularization\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "\n",
        "        # Answer-span prediction heads (start & end positions)\n",
        "        self.qa_start = nn.Linear(hidden_size, 1)\n",
        "        self.qa_end = nn.Linear(hidden_size, 1)\n",
        "\n",
        "        # Context prediction head (binary classification per token)\n",
        "        self.context_head = nn.Linear(hidden_size, 1)\n",
        "\n",
        "        # Optionally freeze BERT's weights\n",
        "        if freeze_bert:\n",
        "            for param in self.bert.parameters():\n",
        "                param.requires_grad = False  # Do not update BERT weights\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        outputs = self.bert(input_ids, attention_mask=attention_mask)\n",
        "        sequence_output = self.dropout(outputs.last_hidden_state)  # Apply dropout\n",
        "\n",
        "        # Predict start & end positions\n",
        "        start_logits = self.qa_start(sequence_output).squeeze(-1)\n",
        "        end_logits = self.qa_end(sequence_output).squeeze(-1)\n",
        "\n",
        "        # Predict context word probabilities (sigmoid activation applied later)\n",
        "        context_logits = self.context_head(sequence_output).squeeze(-1)\n",
        "\n",
        "        return start_logits, end_logits, context_logits\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "vPN1ulSLeNdK"
      },
      "outputs": [],
      "source": [
        "def create_context_labels(input_ids, answer_start, answer_end, window_size=3, decay_factor=0.7):\n",
        "    \"\"\"\n",
        "    Generate soft-labels for context prediction by assigning probabilities to words near the answer span.\n",
        "    \"\"\"\n",
        "    context_labels = np.zeros(len(input_ids))\n",
        "\n",
        "    # Assign 1.0 to words inside the answer span\n",
        "    context_labels[answer_start:answer_end + 1] = 1.0\n",
        "\n",
        "    # Assign decaying probabilities to surrounding words\n",
        "    for i in range(1, window_size + 1):\n",
        "        if answer_start - i >= 0:\n",
        "            context_labels[answer_start - i] = decay_factor ** i\n",
        "        if answer_end + i < len(input_ids):\n",
        "            context_labels[answer_end + i] = decay_factor ** i\n",
        "\n",
        "    return context_labels\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "5UH_DN_deNdL"
      },
      "outputs": [],
      "source": [
        "def compute_loss(start_logits, end_logits, context_logits, start_positions, end_positions, context_labels, lambda_factor=0.8):\n",
        "    \"\"\"\n",
        "    Compute combined loss: Answer span + Context prediction.\n",
        "    \"\"\"\n",
        "    ce_loss = nn.CrossEntropyLoss()\n",
        "    bce_loss = nn.BCEWithLogitsLoss()\n",
        "\n",
        "    # Answer span loss\n",
        "    start_loss = ce_loss(start_logits, start_positions)\n",
        "    end_loss = ce_loss(end_logits, end_positions)\n",
        "    answer_loss = (start_loss + end_loss) / 2\n",
        "\n",
        "    # Context prediction loss\n",
        "    context_loss = bce_loss(context_logits, context_labels.float())\n",
        "\n",
        "    # Weighted sum of both losses\n",
        "    total_loss = (1 - lambda_factor) * answer_loss + lambda_factor * context_loss\n",
        "    return total_loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176,
          "referenced_widgets": [
            "879bfc19ea4b4969a5ba56e21e83befc",
            "7b87a2daec7142859df6dd16c4099b95",
            "930716b920ef410c86a4899cf95a300c",
            "6c97cc60b2d945c698428ebe6966d771",
            "11a64fdc5028431281f0b939bb6edbab",
            "44120fa291324f25b6b2bf3ded431a63",
            "35c618101e87474bba9c6996c4d3a271",
            "21a20885ba67473eaa1b187fc0e25b2a",
            "c1cadfd7c72948f782558adc7cfd874d",
            "e65cfaa805d24562975d68f57a578261",
            "77e9c5e5a90942049c39139a3076f3e6"
          ]
        },
        "id": "unFnnqe8eNdL",
        "outputId": "a2e4049d-e28d-47dd-c749-09a1448f0adc"
      },
      "outputs": [],
      "source": [
        "from transformers import DistilBertTokenizerFast\n",
        "dataset = load_dataset(\"squad\")\n",
        "\n",
        "# Load tokenizer\n",
        "tokenizer = DistilBertTokenizerFast.from_pretrained(\"distilbert-base-uncased\")\n",
        "\n",
        "# Tokenizing function\n",
        "def tokenize_and_align(batch):\n",
        "    \"\"\"\n",
        "    Tokenize question + context together and align answer spans.\n",
        "    \"\"\"\n",
        "    inputs = tokenizer(batch[\"question\"], batch[\"context\"], truncation=True, padding=\"max_length\", max_length=384, return_offsets_mapping=True)\n",
        "\n",
        "    start_positions = []\n",
        "    end_positions = []\n",
        "\n",
        "    for i, offsets in enumerate(inputs[\"offset_mapping\"]):\n",
        "        # Extracting the answer details correctly for each sample in the batch\n",
        "        if len(batch[\"answers\"][i][\"text\"]) == 0:\n",
        "            # No Answer Case\n",
        "            start_positions.append(-100)\n",
        "            end_positions.append(-100)\n",
        "        else:\n",
        "            answer_start = batch[\"answers\"][i][\"answer_start\"][0]  # First answer occurrence\n",
        "            answer_text = batch[\"answers\"][i][\"text\"][0]  # First answer text\n",
        "\n",
        "            # Locate token index corresponding to answer span\n",
        "            start_index = end_index = None\n",
        "            for idx, (start, end) in enumerate(offsets):\n",
        "                if start <= answer_start < end:\n",
        "                    start_index = idx\n",
        "                if start < answer_start + len(answer_text) <= end:\n",
        "                    end_index = idx\n",
        "\n",
        "            # If valid indices were found, append them\n",
        "            if start_index is not None and end_index is not None:\n",
        "                start_positions.append(start_index)\n",
        "                end_positions.append(end_index)\n",
        "            else:\n",
        "                start_positions.append(-100)  # Default to no answer\n",
        "                end_positions.append(-100)\n",
        "\n",
        "    # Store tokenized input and computed start/end positions\n",
        "    inputs[\"start_positions\"] = start_positions\n",
        "    inputs[\"end_positions\"] = end_positions\n",
        "\n",
        "    # Remove offset mapping to avoid errors during training\n",
        "    inputs.pop(\"offset_mapping\")\n",
        "\n",
        "    return inputs\n",
        "\n",
        "# Apply tokenization with batching\n",
        "train_dataset = dataset[\"train\"].map(tokenize_and_align, batched=True)\n",
        "val_dataset = dataset[\"validation\"].map(tokenize_and_align, batched=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['id', 'title', 'context', 'question', 'answers', 'input_ids', 'attention_mask', 'start_positions', 'end_positions'],\n",
              "    num_rows: 87599\n",
              "})"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['id', 'title', 'context', 'question', 'answers', 'input_ids', 'attention_mask', 'start_positions', 'end_positions'],\n",
              "    num_rows: 10570\n",
              "})"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "val_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "c-d2b1AreNdN"
      },
      "outputs": [],
      "source": [
        "class QADataset(Dataset):\n",
        "    def __init__(self, dataset):\n",
        "        self.dataset = dataset\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.dataset[idx]\n",
        "        return {key: torch.tensor(item[key]) for key in [\"input_ids\", \"attention_mask\", \"start_positions\", \"end_positions\"]}\n",
        "\n",
        "# Convert to PyTorch datasets\n",
        "train_dataset = QADataset(train_dataset)\n",
        "val_dataset = QADataset(val_dataset)\n",
        "\n",
        "# Dataloaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=128, shuffle=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Zjow5rg_eNdN"
      },
      "outputs": [],
      "source": [
        "def compute_loss(start_logits, end_logits, start_positions, end_positions):\n",
        "    ce_loss = nn.CrossEntropyLoss(ignore_index=-100)  # Ignore samples with no answer\n",
        "    start_loss = ce_loss(start_logits, start_positions)\n",
        "    end_loss = ce_loss(end_logits, end_positions)\n",
        "    return (start_loss + end_loss) / 2  # Average loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "BgNg_49EeNdO"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "gkqITuNfeNdO"
      },
      "outputs": [],
      "source": [
        "import string\n",
        "import re\n",
        "\n",
        "def normalize_answer(s):\n",
        "    \"\"\"\n",
        "    Lowercases, removes punctuation, articles, and extra whitespace from answers.\n",
        "    \"\"\"\n",
        "    def remove_articles(text):\n",
        "        return re.sub(r'\\b(a|an|the)\\b', ' ', text)\n",
        "\n",
        "    def white_space_fix(text):\n",
        "        return ' '.join(text.split())\n",
        "\n",
        "    def remove_punctuation(text):\n",
        "        return text.translate(str.maketrans('', '', string.punctuation))\n",
        "\n",
        "    return white_space_fix(remove_articles(remove_punctuation(s.lower())))\n",
        "\n",
        "def exact_match(prediction, ground_truth):\n",
        "    \"\"\"\n",
        "    Returns 1 if the predicted answer exactly matches the ground truth, otherwise 0.\n",
        "    \"\"\"\n",
        "    return int(normalize_answer(prediction) == normalize_answer(ground_truth))\n",
        "\n",
        "def f1_score(prediction, ground_truth):\n",
        "    \"\"\"\n",
        "    Compute F1 score between predicted answer and ground truth.\n",
        "    \"\"\"\n",
        "    prediction_tokens = normalize_answer(prediction).split()\n",
        "    ground_truth_tokens = normalize_answer(ground_truth).split()\n",
        "\n",
        "    common_tokens = set(prediction_tokens) & set(ground_truth_tokens)\n",
        "    if len(common_tokens) == 0:\n",
        "        return 0.0\n",
        "\n",
        "    precision = len(common_tokens) / len(prediction_tokens)\n",
        "    recall = len(common_tokens) / len(ground_truth_tokens)\n",
        "    return (2 * precision * recall) / (precision + recall)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bv1SQ3xSeNdO",
        "outputId": "8319e6ea-ddd9-4ee8-89ff-573818e41d36"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1/10 (Training): 100%|██████████| 685/685 [10:06<00:00,  1.13it/s, loss=1.18]\n",
            "Epoch 1/10 (Validation): 100%|██████████| 83/83 [00:31<00:00,  2.66it/s, val_loss=1.52]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1 | Train Loss: 2.3471 | Val Loss: 1.4100 | EM: 0.5550 | F1: 0.7131\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2/10 (Training): 100%|██████████| 685/685 [10:06<00:00,  1.13it/s, loss=1.2]\n",
            "Epoch 2/10 (Validation): 100%|██████████| 83/83 [00:31<00:00,  2.66it/s, val_loss=1.34]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2 | Train Loss: 1.3751 | Val Loss: 1.2624 | EM: 0.5899 | F1: 0.7419\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3/10 (Training): 100%|██████████| 685/685 [10:07<00:00,  1.13it/s, loss=1.26]\n",
            "Epoch 3/10 (Validation): 100%|██████████| 83/83 [00:31<00:00,  2.65it/s, val_loss=1.24]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3 | Train Loss: 1.1879 | Val Loss: 1.1883 | EM: 0.6128 | F1: 0.7608\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 4/10 (Training): 100%|██████████| 685/685 [10:06<00:00,  1.13it/s, loss=1.08]\n",
            "Epoch 4/10 (Validation): 100%|██████████| 83/83 [00:31<00:00,  2.64it/s, val_loss=1.24]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 4 | Train Loss: 1.0646 | Val Loss: 1.1568 | EM: 0.6188 | F1: 0.7668\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 5/10 (Training): 100%|██████████| 685/685 [10:06<00:00,  1.13it/s, loss=0.664]\n",
            "Epoch 5/10 (Validation): 100%|██████████| 83/83 [00:31<00:00,  2.64it/s, val_loss=1.16]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 5 | Train Loss: 0.9651 | Val Loss: 1.1422 | EM: 0.6257 | F1: 0.7726\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 6/10 (Training): 100%|██████████| 685/685 [10:06<00:00,  1.13it/s, loss=1.01]\n",
            "Epoch 6/10 (Validation): 100%|██████████| 83/83 [00:31<00:00,  2.63it/s, val_loss=1.26]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 6 | Train Loss: 0.8812 | Val Loss: 1.1756 | EM: 0.6251 | F1: 0.7739\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 7/10 (Training): 100%|██████████| 685/685 [10:06<00:00,  1.13it/s, loss=0.717]\n",
            "Epoch 7/10 (Validation): 100%|██████████| 83/83 [00:31<00:00,  2.64it/s, val_loss=1.25]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 7 | Train Loss: 0.8070 | Val Loss: 1.1580 | EM: 0.6258 | F1: 0.7753\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 8/10 (Training): 100%|██████████| 685/685 [10:06<00:00,  1.13it/s, loss=0.843]\n",
            "Epoch 8/10 (Validation): 100%|██████████| 83/83 [00:31<00:00,  2.65it/s, val_loss=1.35]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 8 | Train Loss: 0.7405 | Val Loss: 1.2021 | EM: 0.6299 | F1: 0.7771\n",
            "Early stopping triggered! Stopping training.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "model = BLANC(freeze_bert=False).to(device)\n",
        "\n",
        "# optimizer = optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=2e-5, weight_decay=0.01)\n",
        "optimizer = optim.AdamW(model.parameters(), lr=1e-5, weight_decay=0.01)\n",
        "\n",
        "epochs = 10\n",
        "patience = 3  # Stop training if no improvement after 3 epochs\n",
        "best_val_loss = float(\"inf\")\n",
        "patience_counter = 0  # Early stopping counter\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    total_train_loss = 0\n",
        "\n",
        "    progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs} (Training)\")\n",
        "\n",
        "    for batch in progress_bar:\n",
        "        batch = {k: v.to(device) for k, v in batch.items()}\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        start_logits, end_logits, _ = model(batch[\"input_ids\"], batch[\"attention_mask\"])  # Ignore context logits\n",
        "\n",
        "        loss = compute_loss(start_logits, end_logits, batch[\"start_positions\"], batch[\"end_positions\"])\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_train_loss += loss.item()\n",
        "\n",
        "        progress_bar.set_postfix(loss=loss.item())\n",
        "\n",
        "    avg_train_loss = total_train_loss / len(train_loader)\n",
        "\n",
        "    # Validation \n",
        "    model.eval()\n",
        "    total_val_loss = 0\n",
        "    total_em, total_f1, num_samples = 0, 0, 0  # Initialize F1 & EM counters\n",
        "\n",
        "    val_progress_bar = tqdm(val_loader, desc=f\"Epoch {epoch+1}/{epochs} (Validation)\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in val_progress_bar:\n",
        "            batch = {k: v.to(device) for k, v in batch.items()}\n",
        "\n",
        "            start_logits, end_logits, _ = model(batch[\"input_ids\"], batch[\"attention_mask\"])\n",
        "\n",
        "            loss = compute_loss(start_logits, end_logits, batch[\"start_positions\"], batch[\"end_positions\"])\n",
        "            total_val_loss += loss.item()\n",
        "\n",
        "            for i in range(batch[\"input_ids\"].shape[0]):  \n",
        "                input_ids = batch[\"input_ids\"][i]\n",
        "                start_pred = torch.argmax(start_logits[i]).item()\n",
        "                end_pred = torch.argmax(end_logits[i]).item()\n",
        "\n",
        "                # Ensure start ≤ end\n",
        "                if start_pred > end_pred:\n",
        "                    continue\n",
        "\n",
        "                predicted_answer = tokenizer.decode(input_ids[start_pred:end_pred + 1])\n",
        "\n",
        "                # Get ground truth answer\n",
        "                start_true = batch[\"start_positions\"][i].item()\n",
        "                end_true = batch[\"end_positions\"][i].item()\n",
        "\n",
        "                if start_true == -100 or end_true == -100:\n",
        "                    continue  \n",
        "\n",
        "                true_answer = tokenizer.decode(input_ids[start_true:end_true + 1])\n",
        "\n",
        "                # Computing F1 and EM\n",
        "                total_em += exact_match(predicted_answer, true_answer)\n",
        "                total_f1 += f1_score(predicted_answer, true_answer)\n",
        "                num_samples += 1\n",
        "\n",
        "            val_progress_bar.set_postfix(val_loss=loss.item())\n",
        "\n",
        "    avg_val_loss = total_val_loss / len(val_loader)\n",
        "    avg_f1 = total_f1 / num_samples\n",
        "    avg_em = total_em / num_samples\n",
        "\n",
        "    print(f\"Epoch {epoch+1} | Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f} | EM: {avg_em:.4f} | F1: {avg_f1:.4f}\")\n",
        "\n",
        "    # Early stopping \n",
        "    if avg_val_loss < best_val_loss:\n",
        "        best_val_loss = avg_val_loss\n",
        "        patience_counter = 0  # Reseting\n",
        "    else:\n",
        "        patience_counter += 1\n",
        "        if patience_counter >= patience:\n",
        "            print(\"Early stopping triggered! Stopping training.\")\n",
        "            break  # Stop training\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "IE5S18GAeNdP"
      },
      "outputs": [],
      "source": [
        "torch.save(model.state_dict(), \"qa_squad2_final.pth\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0hB45F-L-fQ2",
        "outputId": "df35a381-80a3-4d06-e8b1-aa05f6eb0be0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "baku\n"
          ]
        }
      ],
      "source": [
        "def predict_answer(model, question, context):\n",
        "    model.eval()\n",
        "\n",
        "    # Tokenize input\n",
        "    inputs = tokenizer(question, context, return_tensors=\"pt\", truncation=True, padding=True).to(device)\n",
        "\n",
        "    # Forward pass\n",
        "    start_logits, end_logits, context_logits = model(inputs[\"input_ids\"], inputs[\"attention_mask\"])\n",
        "\n",
        "    # Get predicted answer span\n",
        "    start_idx = torch.argmax(start_logits)\n",
        "    end_idx = torch.argmax(end_logits)\n",
        "\n",
        "    # Extract predicted answer\n",
        "    answer_tokens = inputs[\"input_ids\"][0, start_idx:end_idx + 1]\n",
        "    predicted_answer = tokenizer.decode(answer_tokens)\n",
        "\n",
        "    return predicted_answer\n",
        "\n",
        "print(predict_answer(model, \"What is the capital of Azerbaijan?\", \"It's Baku, full of rich history\"))\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "11a64fdc5028431281f0b939bb6edbab": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "21a20885ba67473eaa1b187fc0e25b2a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "35c618101e87474bba9c6996c4d3a271": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "44120fa291324f25b6b2bf3ded431a63": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6c97cc60b2d945c698428ebe6966d771": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e65cfaa805d24562975d68f57a578261",
            "placeholder": "​",
            "style": "IPY_MODEL_77e9c5e5a90942049c39139a3076f3e6",
            "value": " 10570/10570 [00:03&lt;00:00, 2680.87 examples/s]"
          }
        },
        "77e9c5e5a90942049c39139a3076f3e6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7b87a2daec7142859df6dd16c4099b95": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_44120fa291324f25b6b2bf3ded431a63",
            "placeholder": "​",
            "style": "IPY_MODEL_35c618101e87474bba9c6996c4d3a271",
            "value": "Map: 100%"
          }
        },
        "879bfc19ea4b4969a5ba56e21e83befc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7b87a2daec7142859df6dd16c4099b95",
              "IPY_MODEL_930716b920ef410c86a4899cf95a300c",
              "IPY_MODEL_6c97cc60b2d945c698428ebe6966d771"
            ],
            "layout": "IPY_MODEL_11a64fdc5028431281f0b939bb6edbab"
          }
        },
        "930716b920ef410c86a4899cf95a300c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_21a20885ba67473eaa1b187fc0e25b2a",
            "max": 10570,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c1cadfd7c72948f782558adc7cfd874d",
            "value": 10570
          }
        },
        "c1cadfd7c72948f782558adc7cfd874d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e65cfaa805d24562975d68f57a578261": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
